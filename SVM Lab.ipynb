{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.01 - Supervised Learning Model Comparison\n",
    "\n",
    "Recall the \"data science process.\"\n",
    "\n",
    "1. Define the problem.\n",
    "2. Gather the data.\n",
    "3. Explore the data.\n",
    "4. Model the data.\n",
    "5. Evaluate the model.\n",
    "6. Answer the problem.\n",
    "\n",
    "In this lab, we're going to focus mostly on creating (and then comparing) many regression and classification models. Thus, we'll define the problem and gather the data for you.\n",
    "Most of the questions requiring a written response can be written in 2-3 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the problem.\n",
    "\n",
    "You are a data scientist with a financial services company. Specifically, you want to leverage data in order to identify potential customers.\n",
    "\n",
    "If you are unfamiliar with \"401(k)s\" or \"IRAs,\" these are two types of retirement accounts. Very broadly speaking:\n",
    "- You can put money for retirement into both of these accounts.\n",
    "- The money in these accounts gets invested and hopefully has a lot more money in it when you retire.\n",
    "- These are a little different from regular bank accounts in that there are certain tax benefits to these accounts. Also, employers frequently match money that you put into a 401k.\n",
    "- If you want to learn more about them, check out [this site](https://www.nerdwallet.com/article/ira-vs-401k-retirement-accounts).\n",
    "\n",
    "We will tackle one regression problem and one classification problem today.\n",
    "- Regression: What features best predict one's income?\n",
    "- Classification: Predict whether or not one is eligible for a 401k.\n",
    "\n",
    "Check out the data dictionary [here](http://fmwww.bc.edu/ec-p/data/wooldridge2k/401KSUBS.DES).\n",
    "\n",
    "### NOTE: When predicting `inc`, you should pretend as though you do not have access to the `e401k`, the `p401k` variable, and the `pira` variable. When predicting `e401k`, you may use the entire dataframe if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import BaggingRegressor, BaggingClassifier, RandomForestRegressor, RandomForestClassifier, AdaBoostRegressor, AdaBoostClassifier\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVR, SVC\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Gather the data.\n",
    "\n",
    "##### 1. Read in the data from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e401k</th>\n",
       "      <th>inc</th>\n",
       "      <th>marr</th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>fsize</th>\n",
       "      <th>nettfa</th>\n",
       "      <th>p401k</th>\n",
       "      <th>pira</th>\n",
       "      <th>incsq</th>\n",
       "      <th>agesq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>13.170</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>4.575</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>173.4489</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>61.230</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>154.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3749.1130</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>12.858</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>165.3282</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>98.880</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>21.800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9777.2540</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>22.614</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>18.450</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>511.3930</td>\n",
       "      <td>2809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   e401k     inc  marr  male  age  fsize   nettfa  p401k  pira      incsq  \\\n",
       "0      0  13.170     0     0   40      1    4.575      0     1   173.4489   \n",
       "1      1  61.230     0     1   35      1  154.000      1     0  3749.1130   \n",
       "2      0  12.858     1     0   44      2    0.000      0     0   165.3282   \n",
       "3      0  98.880     1     1   44      2   21.800      0     0  9777.2540   \n",
       "4      0  22.614     0     0   53      1   18.450      0     0   511.3930   \n",
       "\n",
       "   agesq  \n",
       "0   1600  \n",
       "1   1225  \n",
       "2   1936  \n",
       "3   1936  \n",
       "4   2809  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('401ksubs.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. What are 2-3 other variables that, if available, would be helpful to have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- It would be helpful to have variables that predict financial stability, such as a) how much money is in one's savings account, b) how much money is in one's checking account, and c) what is one's credit score.\n",
    "- Owning a house versus renting a residence may be a good indicator of someone able and willing to save money. (The same goes for a car, too!)\n",
    "- Knowing whether or not the person has made investments into stocks, bonds, or Certificates of Deposits (CDs) would be helpful to predict eligibility for 401(k)s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Suppose a peer recommended putting `race` into your model in order to better predict who to target when advertising IRAs and 401(k)s. Why would this be an unethical decision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " It's unethical to use race to determine who should qualify for any particiular product or service. Someone should not be disqualified from a model on the basis of race as someone's race does not determine their eligibility for 401k's and related finicial qualifications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Explore the data.\n",
    "\n",
    "##### 4. When attempting to predict income, which feature(s) would we reasonably not use? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: When predicting income I will obviously not be needing or using the income or income squared columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. What two variables have already been created for us through feature engineering? Come up with a hypothesis as to why subject-matter experts may have done this.\n",
    "> This need not be a \"statistical hypothesis.\" Just brainstorm why SMEs might have done this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer: Age and income have both been squared i.e., feature engineered. Assuming that this was done by subject-matter experts, it's likely that they found there's some quadratic (squared) relationship between income and eligibility for a 401k. (The same goes for age and eligibility for a 401k.) Perhaps people who are older or have higher incomes are exponentially more likely to be eligible for an IRA, so these terms account for that nonlinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Looking at the data dictionary, one variable description appears to be an error. What is this error, and what do you think the correct value would be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer: The income column is described as income squared when that does not appear to be the case, similarly  the age column is also described as age squared when it just is age. There is also a seperate income squared and seperate age squared column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model the data. (Part 1: Regression Problem)\n",
    "\n",
    "Recall:\n",
    "- Problem: What features best predict one's income?\n",
    "- When predicting `inc`, you should pretend as though you do not have access to the `e401k`, the `p401k` variable, and the `pira` variable.\n",
    "\n",
    "##### 7. List all modeling tactics we've learned that could be used to solve a regression problem (as of Wednesday afternoon of Week 6). For each tactic, identify whether it is or is not appropriate for solving this specific regression problem and explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "- a multiple linear regression model (Yes, we can understand influence of features.)\n",
    "- a $k$-nearest neighbors model (No, we cannot understand influence of features.)\n",
    "- a decision tree (Yes, we can understand influence of features.)\n",
    "- a set of bagged decision trees (Yes, we can understand influence of features.)\n",
    "- a random forest (Yes, we can understand influence of features.)\n",
    "- a set of extremely randomized trees (Yes, we can understand influence of features.)\n",
    "- an Adaboost model (Yes, we can understand influence of features.)\n",
    "- an XGBoost model (Yes, we can understand influence of features.)\n",
    "- a support vector regressor (Yes, we can understand influence of features.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Regardless of your answer to number 7, fit at least one of each of the following models to attempt to solve the regression problem above:\n",
    "    - a multiple linear regression model\n",
    "    - a k-nearest neighbors model\n",
    "    - a decision tree\n",
    "    - a set of bagged decision trees\n",
    "    - a random forest\n",
    "    - an Adaboost model\n",
    "    - a support vector regressor\n",
    "    \n",
    "> As always, be sure to do a train/test split! In order to compare modeling techniques, you should use the same train-test split on each. I recommend setting a random seed here.\n",
    "\n",
    "> You may find it helpful to set up a pipeline to try each modeling technique, but you are not required to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e401k</th>\n",
       "      <th>inc</th>\n",
       "      <th>marr</th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>fsize</th>\n",
       "      <th>nettfa</th>\n",
       "      <th>p401k</th>\n",
       "      <th>pira</th>\n",
       "      <th>incsq</th>\n",
       "      <th>agesq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.803173</td>\n",
       "      <td>-1.082858</td>\n",
       "      <td>-1.300887</td>\n",
       "      <td>-0.506898</td>\n",
       "      <td>-0.104886</td>\n",
       "      <td>-1.235500</td>\n",
       "      <td>-0.226651</td>\n",
       "      <td>-0.617776</td>\n",
       "      <td>1.712236</td>\n",
       "      <td>-0.648965</td>\n",
       "      <td>-0.216227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.245062</td>\n",
       "      <td>0.912268</td>\n",
       "      <td>-1.300887</td>\n",
       "      <td>1.972784</td>\n",
       "      <td>-0.590372</td>\n",
       "      <td>-1.235500</td>\n",
       "      <td>2.109561</td>\n",
       "      <td>1.618709</td>\n",
       "      <td>-0.584032</td>\n",
       "      <td>0.542404</td>\n",
       "      <td>-0.634940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.803173</td>\n",
       "      <td>-1.095810</td>\n",
       "      <td>0.768706</td>\n",
       "      <td>-0.506898</td>\n",
       "      <td>0.283503</td>\n",
       "      <td>-0.580086</td>\n",
       "      <td>-0.298179</td>\n",
       "      <td>-0.617776</td>\n",
       "      <td>-0.584032</td>\n",
       "      <td>-0.651671</td>\n",
       "      <td>0.158941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.803173</td>\n",
       "      <td>2.475242</td>\n",
       "      <td>0.768706</td>\n",
       "      <td>1.972784</td>\n",
       "      <td>0.283503</td>\n",
       "      <td>-0.580086</td>\n",
       "      <td>0.042656</td>\n",
       "      <td>-0.617776</td>\n",
       "      <td>-0.584032</td>\n",
       "      <td>2.550909</td>\n",
       "      <td>0.158941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.803173</td>\n",
       "      <td>-0.690807</td>\n",
       "      <td>-1.300887</td>\n",
       "      <td>-0.506898</td>\n",
       "      <td>1.157377</td>\n",
       "      <td>-1.235500</td>\n",
       "      <td>-0.009720</td>\n",
       "      <td>-0.617776</td>\n",
       "      <td>-0.584032</td>\n",
       "      <td>-0.536366</td>\n",
       "      <td>1.133706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      e401k       inc      marr      male       age     fsize    nettfa  \\\n",
       "0 -0.803173 -1.082858 -1.300887 -0.506898 -0.104886 -1.235500 -0.226651   \n",
       "1  1.245062  0.912268 -1.300887  1.972784 -0.590372 -1.235500  2.109561   \n",
       "2 -0.803173 -1.095810  0.768706 -0.506898  0.283503 -0.580086 -0.298179   \n",
       "3 -0.803173  2.475242  0.768706  1.972784  0.283503 -0.580086  0.042656   \n",
       "4 -0.803173 -0.690807 -1.300887 -0.506898  1.157377 -1.235500 -0.009720   \n",
       "\n",
       "      p401k      pira     incsq     agesq  \n",
       "0 -0.617776  1.712236 -0.648965 -0.216227  \n",
       "1  1.618709 -0.584032  0.542404 -0.634940  \n",
       "2 -0.617776 -0.584032 -0.651671  0.158941  \n",
       "3 -0.617776 -0.584032  2.550909  0.158941  \n",
       "4 -0.617776 -0.584032 -0.536366  1.133706  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scale our dataframe \n",
    "\n",
    "scaled_df = pd.DataFrame(StandardScaler().fit_transform(df),\n",
    "                        columns = df.columns)\n",
    "\n",
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaled_df.drop(columns = ['e401k', 'p401k', 'pira', 'inc', 'incsq'])\n",
    "\n",
    "y = scaled_df['inc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using all features besides inc, inc squared and 401k vars.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_df.drop(columns = ['e401k', 'p401k', 'pira', 'inc', 'incsq']),\n",
    "                                                    scaled_df['inc'],\n",
    "                                                    test_size = .2,\n",
    "                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train, y_train)\n",
    "\n",
    "knn_reg = KNeighborsRegressor()\n",
    "knn_reg.fit(X_train, y_train)\n",
    "\n",
    "cart_reg = DecisionTreeRegressor()\n",
    "cart_reg.fit(X_train, y_train)\n",
    "\n",
    "bagged_reg = BaggingRegressor()\n",
    "bagged_reg.fit(X_train, y_train)\n",
    "\n",
    "random_forest_reg = RandomForestRegressor()\n",
    "random_forest_reg.fit(X_train, y_train)\n",
    "\n",
    "adaboost_reg = AdaBoostRegressor()\n",
    "adaboost_reg.fit(X_train, y_train)\n",
    "\n",
    "support_vector_reg = SVR()\n",
    "support_vector_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. What is bootstrapping?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer: Bootstrapping is random resampling with reaplcement. You take an original data sample, take many sub-samples with replacement, and those are bootstrapped samples.  We usually use it in order to simulate many different samples or to empirically estimate the sampling distribution of a statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. What is the difference between a decision tree and a set of bagged decision trees? Be specific and precise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer: With a set of bagged decision trees, we have bootstrapped $k$ different samples and grown one decision tree on each bootstrapped sample, then our predictions are aggregated. With one decision tree, we only use the original sample and grow exactly one decision tree. No aggregation of predictions occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11. What is the difference between a set of bagged decision trees and a random forest? Be specific and precise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer: In bagged decision trees, every variable is considered as a \"candidate\" for splitting at each node in the decision tree. In random forests, a random subset of variables is considered as candidataes for splitting at each node. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12. Why might a random forest be superior to a set of bagged decision trees?\n",
    "> Hint: Consider the bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer: In a random forest, we randomly select which features go into each split. This effectively makes our individual decision trees less correlated, which decreases the variance of our predictions after aggregating the different decision trees. Thus, a random forest usually has less variance than bagged decision trees.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the model. (Part 1: Regression Problem)\n",
    "\n",
    "##### 13. Using RMSE, evaluate each of the models you fit on both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_score(model, X_train, X_test, y_train, y_test):\n",
    "    mse_train = mean_squared_error(y_true = y_train,\n",
    "                                  y_pred = model.predict(X_train))\n",
    "    mse_test = mean_squared_error(y_true = y_test,\n",
    "                                  y_pred = model.predict(X_test))\n",
    "    rmse_train = mse_train ** 0.5\n",
    "    rmse_test = mse_test ** 0.5\n",
    "    \n",
    "    print(\"The training RMSE for \" + str(model) + \" is: \" + str(rmse_train))\n",
    "    print(\"The testing RMSE for \" + str(model) + \" is: \" + str(rmse_test))\n",
    "    return (rmse_train, rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training RMSE for LinearRegression() is: 0.8370830332453489\n",
      "The testing RMSE for LinearRegression() is: 0.8675193605893167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8370830332453489, 0.8675193605893167)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Linear Regression\n",
    "rmse_score(linear_reg, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training RMSE for KNeighborsRegressor() is: 0.6860905782308746\n",
      "The testing RMSE for KNeighborsRegressor() is: 0.8373266164049329\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6860905782308746, 0.8373266164049329)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#KNN \n",
    "rmse_score(knn_reg, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training RMSE for DecisionTreeRegressor() is: 0.09397820060704348\n",
      "The testing RMSE for DecisionTreeRegressor() is: 1.1247456481932119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.09397820060704348, 1.1247456481932119)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Decision Tree \n",
    "\n",
    "\n",
    "rmse_score(cart_reg, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training RMSE for BaggingRegressor() is: 0.3633811622127923\n",
      "The testing RMSE for BaggingRegressor() is: 0.874687992042935\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3633811622127923, 0.874687992042935)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bagging Regressor\n",
    "\n",
    "rmse_score(bagged_reg, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training RMSE for RandomForestRegressor() is: 0.32040091355927897\n",
      "The testing RMSE for RandomForestRegressor() is: 0.8427712208089381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.32040091355927897, 0.8427712208089381)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random Forests\n",
    "\n",
    "rmse_score(random_forest_reg, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training RMSE for AdaBoostRegressor() is: 0.9206854207488931\n",
      "The testing RMSE for AdaBoostRegressor() is: 0.960467791770503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9206854207488931, 0.960467791770503)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#AdaBoost \n",
    "\n",
    "rmse_score(adaboost_reg, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training RMSE for SVR() is: 0.7858885593080802\n",
      "The testing RMSE for SVR() is: 0.8206525603025999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7858885593080802, 0.8206525603025999)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVR \n",
    "rmse_score(support_vector_reg, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the training and testing RMSE in a table so we can directly compare them all:\n",
    "\n",
    "|           Model          | Training RMSE | Testing RMSE |\n",
    "|:------------------------:|:-------------:|:------------:|\n",
    "|     Linear Regression    |     0.837     |     0.868    |\n",
    "|    k-Nearest Neighbors   |     0.686     |     0.837    |\n",
    "|       Decision Tree      |     0.093     |     1.124    |\n",
    "|   Bagged Decision Trees  |     0.363     |     0.874    |\n",
    "|       Random Forest      |     0.320     |     0.842    |\n",
    "|         AdaBoost         |     0.902     |     0.960    |\n",
    "| Support Vector Regressor |     0.786     |     0.820    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 14. Based on training RMSE and testing RMSE, is there evidence of overfitting in any of your models? Which ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "Answer: It appears that every model is overfit to the training data. We recognize this because our testing RMSE is worse (higher) than our training RMSE. Our models are generalizing poorly to held-out/unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 15. Based on everything we've covered so far, if you had to pick just one model as your final model to use to answer the problem in front of you, which one model would you pick? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I would pick AdaBoost or Linear Regression on the basis that the scores are more consistent across train and test when compared to other models. LinearRegression is similer to understand yet AdaBoost yeilds higher scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 16. Suppose you wanted to improve the performance of your final model. Brainstorm 2-3 things that, if you had more time, you would attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Tune Models hyperparamters using GridSearch \n",
    "- Further feature engineering of the data \n",
    "- Transform y variable further. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model the data. (Part 2: Classification Problem)\n",
    "\n",
    "Recall:\n",
    "- Problem: Predict whether or not one is eligible for a 401k.\n",
    "- When predicting `e401k`, you may use the entire dataframe if you wish.\n",
    "\n",
    "##### 17. While you're allowed to use every variable in your dataframe, mention at least one disadvantage of using `p401k` in your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer: We're predicting whether or not someone is eligible for a 401k. If we include whether or not someone already possesses a 401k, then every person with a 401k must by definition be eligible for a 401k. However, this probably doesn't contribute to our understanding of what makes someone eligible for a 401k and will only confound what we're actually interested in studying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 18. List all modeling tactics we've learned that could be used to solve a classification problem (as of Wednesday afternoon of Week 6). For each tactic, identify whether it is or is not appropriate for solving this specific classification problem and explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- a logistic regression model (Yes, we can predict whether or not one is eligible for a 401(k).)\n",
    "- a $k$-nearest neighbors model (Yes, we can predict whether or not one is eligible for a 401(k).)\n",
    "- a Naive Bayes model (Yes, we can predict whether or not one is eligible for a 401(k).)\n",
    "- a decision tree (Yes, we can predict whether or not one is eligible for a 401(k).)\n",
    "- a set of bagged decision trees (Yes, we can predict whether or not one is eligible for a 401(k).)\n",
    "- a random forest (Yes, we can predict whether or not one is eligible for a 401(k).)\n",
    "- a set of extremely randomized trees (Yes, we can predict whether or not one is eligible for a 401(k).)\n",
    "- an Adaboost model (Yes, we can predict whether or not one is eligible for a 401(k).)\n",
    "- an XGBoost model (Yes, we can predict whether or not one is eligible for a 401(k).)\n",
    "- a support vector classifier (Yes, we can predict whether or not one is eligible for a 401(k).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19. Regardless of your answer to number 18, fit at least one of each of the following models to attempt to solve the classification problem above:\n",
    "    - a logistic regression model\n",
    "    - a k-nearest neighbors model\n",
    "    - a decision tree\n",
    "    - a set of bagged decision trees\n",
    "    - a random forest\n",
    "    - an Adaboost model\n",
    "    - a support vector classifier\n",
    "    \n",
    "> As always, be sure to do a train/test split! In order to compare modeling techniques, you should use the same train-test split on each. I recommend using a random seed here.\n",
    "\n",
    "> You may find it helpful to set up a pipeline to try each modeling technique, but you are not required to do so!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_df.drop(columns = ['e401k', 'p401k']),\n",
    "                                                    [1 if scaled_df['e401k'][i] > 0 else 0 for i in range(scaled_df.shape[0])],\n",
    "                                                    ## I ran the above line because when I scaled e401k, e401k was no longer binary!\n",
    "                                                    ## I needed to turn my Y vector into a discrete variable.\n",
    "                                                    test_size = .2,\n",
    "                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#models \n",
    "\n",
    "logreg_class = LogisticRegression()\n",
    "logreg_class.fit(X_train, y_train)\n",
    "\n",
    "knn_class = KNeighborsClassifier()\n",
    "knn_class.fit(X_train, y_train)\n",
    "\n",
    "cart_class = DecisionTreeClassifier()\n",
    "cart_class.fit(X_train, y_train)\n",
    "\n",
    "bagged_class = BaggingClassifier()\n",
    "bagged_class.fit(X_train, y_train)\n",
    "\n",
    "random_forest_class = RandomForestClassifier()\n",
    "random_forest_class.fit(X_train, y_train)\n",
    "\n",
    "adaboost_class = AdaBoostClassifier()\n",
    "adaboost_class.fit(X_train, y_train)\n",
    "\n",
    "support_vector_class = SVC()\n",
    "support_vector_class.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the model. (Part 2: Classfication Problem)\n",
    "\n",
    "##### 20. Suppose our \"positive\" class is that someone is eligible for a 401(k). What are our false positives? What are our false negatives?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negative (True Non - Elibility for 401K): 1272\n",
      "True Positive (True Eligibility for 401K ): 259\n",
      "False Negative (Model says no eligile for 401k, but is actually eligible ): 638\n",
      "False Positive (Model says is eligiile for 401 k but is not): 150\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "\n",
    "print(f\"True Negative (True Non - Elibility for 401K): {tn}\")\n",
    "print(f\"True Positive (True Eligibility for 401K ): {tp}\")\n",
    "print(f\"False Negative (Model says no eligile for 401k, but is actually eligible ): {fn}\")\n",
    "print(f\"False Positive (Model says is eligiile for 401 k but is not): {fp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21. In this specific case, would we rather minimize false positives or minimize false negatives? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We would rather mnimize false positives because we want as many people who are eligible for 401k's to recieve advertising for 401k's. Here we are wanting to minimize false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22. Suppose we wanted to optimize for the answer you provided in problem 21. Which metric would we optimize in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Since we want to optimize for false negatives, we should optimize sensitvity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 23. Suppose that instead of optimizing for the metric in problem 21, we wanted to balance our false positives and false negatives using `f1-score`. Why might [f1-score](https://en.wikipedia.org/wiki/F1_score) be an appropriate metric to use here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The f1 score conveys the balance between the precision and the recall. This will measure our models accuracy on two different metrics which should help it balance out. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24. Using f1-score, evaluate each of the models you fit on both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_scorer(model, X_train, X_test, y_train, y_test):\n",
    "    f1_train = f1_score(y_true = y_train,\n",
    "                        y_pred = model.predict(X_train))\n",
    "    f1_test = f1_score(y_true = y_test,\n",
    "                       y_pred = model.predict(X_test))\n",
    "    \n",
    "    print(\"The training F1-score for \" + str(model) + \" is: \" + str(f1_train))\n",
    "    print(\"The testing F1-score for \" + str(model) + \" is: \" + str(f1_test))\n",
    "    return (f1_train, f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training F1-score for LogisticRegression() is: 0.4727870199219552\n",
      "The testing F1-score for LogisticRegression() is: 0.4777870913663035\n",
      "(0.4727870199219552, 0.4777870913663035)\n",
      "\n",
      "The training F1-score for KNeighborsClassifier() is: 0.653122648607976\n",
      "The testing F1-score for KNeighborsClassifier() is: 0.4977511244377811\n",
      "(0.653122648607976, 0.4977511244377811)\n",
      "\n",
      "The training F1-score for DecisionTreeClassifier() is: 1.0\n",
      "The testing F1-score for DecisionTreeClassifier() is: 0.4654696132596685\n",
      "(1.0, 0.4654696132596685)\n",
      "\n",
      "The training F1-score for BaggingClassifier() is: 0.9678891033514652\n",
      "The testing F1-score for BaggingClassifier() is: 0.49413604378420645\n",
      "(0.9678891033514652, 0.49413604378420645)\n",
      "\n",
      "The training F1-score for RandomForestClassifier() is: 1.0\n",
      "The testing F1-score for RandomForestClassifier() is: 0.5329295987887964\n",
      "(1.0, 0.5329295987887964)\n",
      "\n",
      "The training F1-score for AdaBoostClassifier() is: 0.5621436716077537\n",
      "The testing F1-score for AdaBoostClassifier() is: 0.5688487584650113\n",
      "(0.5621436716077537, 0.5688487584650113)\n",
      "\n",
      "The training F1-score for SVC() is: 0.47162162162162163\n",
      "The testing F1-score for SVC() is: 0.45207956600361665\n",
      "(0.47162162162162163, 0.45207956600361665)\n"
     ]
    }
   ],
   "source": [
    "print(f1_scorer(logreg_class, X_train, X_test, y_train, y_test))\n",
    "print()\n",
    "print(f1_scorer(knn_class, X_train, X_test, y_train, y_test))\n",
    "print()\n",
    "print(f1_scorer(cart_class, X_train, X_test, y_train, y_test))\n",
    "print()\n",
    "print(f1_scorer(bagged_class, X_train, X_test, y_train, y_test))\n",
    "print()\n",
    "print(f1_scorer(random_forest_class, X_train, X_test, y_train, y_test))\n",
    "print()\n",
    "print(f1_scorer(adaboost_class, X_train, X_test, y_train, y_test))\n",
    "print()\n",
    "print(f1_scorer(support_vector_class, X_train, X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25. Based on training f1-score and testing f1-score, is there evidence of overfitting in any of your models? Which ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer: We want our $F_1$ score to be as high as possible. Thus, overfitting occurs when we have a high training $F_1$ score and a low testing $F_1$ score. Models that appear to be overfit are the $k$-nearest neighbors classifier, the decision tree, the bagged decision trees, the random forest, and (slightly) the support vector classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26. Based on everything we've covered so far, if you had to pick just one model as your final model to use to answer the problem in front of you, which one model would you pick? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I would choose Adaboost. Adaboost has shown consistent scores accross train and test data and has relatively high R-sqaured values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27. Suppose you wanted to improve the performance of your final model. Brainstorm 2-3 things that, if you had more time, you would attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Hyperparameter tuning \n",
    "- Feature Engineering\n",
    "- Using other libraries such a polynomial features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Answer the problem.\n",
    "\n",
    "##### BONUS: Briefly summarize your answers to the regression and classification problems. Be sure to include any limitations or hesitations in your answer.\n",
    "\n",
    "- Regression: What features best predict one's income?\n",
    "- Classification: Predict whether or not one is eligible for a 401k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['marr', 'male', 'age', 'fsize', 'nettfa']\n",
    "\n",
    "y = df['inc']\n",
    "\n",
    "X = df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>const</th>\n",
       "      <th>marr</th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>fsize</th>\n",
       "      <th>nettfa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>4.575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>154.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>21.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>18.450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   const  marr  male  age  fsize   nettfa\n",
       "0    1.0     0     0   40      1    4.575\n",
       "1    1.0     0     1   35      1  154.000\n",
       "2    1.0     1     0   44      2    0.000\n",
       "3    1.0     1     1   44      2   21.800\n",
       "4    1.0     0     0   53      1   18.450"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = sm.add_constant(X)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>inc</td>       <th>  R-squared:         </th> <td>   0.264</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.263</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   663.6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 01 Feb 2021</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>10:45:06</td>     <th>  Log-Likelihood:    </th> <td> -41252.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  9275</td>      <th>  AIC:               </th> <td>8.252e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  9269</td>      <th>  BIC:               </th> <td>8.256e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>  <td>   25.8851</td> <td>    1.060</td> <td>   24.422</td> <td> 0.000</td> <td>   23.807</td> <td>   27.963</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>marr</th>   <td>   20.3100</td> <td>    0.558</td> <td>   36.422</td> <td> 0.000</td> <td>   19.217</td> <td>   21.403</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>male</th>   <td>    3.4407</td> <td>    0.582</td> <td>    5.907</td> <td> 0.000</td> <td>    2.299</td> <td>    4.582</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>    <td>    0.0380</td> <td>    0.022</td> <td>    1.766</td> <td> 0.077</td> <td>   -0.004</td> <td>    0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fsize</th>  <td>   -1.4244</td> <td>    0.174</td> <td>   -8.200</td> <td> 0.000</td> <td>   -1.765</td> <td>   -1.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nettfa</th> <td>    0.1284</td> <td>    0.003</td> <td>   37.250</td> <td> 0.000</td> <td>    0.122</td> <td>    0.135</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>2341.739</td> <th>  Durbin-Watson:     </th> <td>   1.828</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>10549.893</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.163</td>  <th>  Prob(JB):          </th> <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 7.679</td>  <th>  Cond. No.          </th> <td>    350.</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    inc   R-squared:                       0.264\n",
       "Model:                            OLS   Adj. R-squared:                  0.263\n",
       "Method:                 Least Squares   F-statistic:                     663.6\n",
       "Date:                Mon, 01 Feb 2021   Prob (F-statistic):               0.00\n",
       "Time:                        10:45:06   Log-Likelihood:                -41252.\n",
       "No. Observations:                9275   AIC:                         8.252e+04\n",
       "Df Residuals:                    9269   BIC:                         8.256e+04\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         25.8851      1.060     24.422      0.000      23.807      27.963\n",
       "marr          20.3100      0.558     36.422      0.000      19.217      21.403\n",
       "male           3.4407      0.582      5.907      0.000       2.299       4.582\n",
       "age            0.0380      0.022      1.766      0.077      -0.004       0.080\n",
       "fsize         -1.4244      0.174     -8.200      0.000      -1.765      -1.084\n",
       "nettfa         0.1284      0.003     37.250      0.000       0.122       0.135\n",
       "==============================================================================\n",
       "Omnibus:                     2341.739   Durbin-Watson:                   1.828\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            10549.893\n",
       "Skew:                           1.163   Prob(JB):                         0.00\n",
       "Kurtosis:                       7.679   Cond. No.                         350.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Based on my summary statistics and their respective p-values, it appears that NETTFA likely is the largest predicting variable of income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
